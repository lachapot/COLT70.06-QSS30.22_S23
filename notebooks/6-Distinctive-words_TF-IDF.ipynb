{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "847ead97-97d3-4997-81a3-377891bffb58",
   "metadata": {},
   "source": [
    "# Identifying distinctive words with Term Frequency-Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d33985-43f3-4293-ac6d-9c7fb2d33e6e",
   "metadata": {},
   "source": [
    "Tf-idf is a method that builds off word frequency but it more specifically tries to identify the most distinctively frequent and significant words in a document compared to a larger set of documents.\n",
    "\n",
    "With TF-IDF each term is weighted by dividing the frequency of term in a document by the number of documents in the corpus containing the word. It gives weight to terms that appear in a document but are rare or absent in other documents.\n",
    "\n",
    "TF-IDF is calculated by taking the number of times a term occurs in a document (term frequency). Then taking the number of documents in which the same term occurs at least once divided by the total number of documents (document frequency), and that fraction is flipped on its head (inverse document frequency =  log((1 + total_number_of_documents) / (number_of_documents_with_term +1)) + 1). Then you multiply the two numbers together (term_frequency * inverse_document_frequency). The reason we take the inverse, or flipped fraction, of document frequency is to boost the rarer words that occur in relatively few documents.\n",
    "\n",
    "\n",
    "In this notebook we use the implemention of tf-idf in Scikit-learn (sklearn). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289fd841-5d57-480a-af89-4cdebb65391c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries we're going to use\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "from pathlib import Path  \n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1980581-ceea-4290-8f6d-efa2569d78bc",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up path to files \n",
    "# and a variable with the file names called text_titles\n",
    "#(we use these later to create our pandas dataframe)\n",
    "directory_path = 'soderberg-corpus'\n",
    "text_files = glob.glob(f'{directory_path}/*.txt')\n",
    "text_titles = [Path(text).stem for text in text_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1d3429-5462-4e9a-83cd-953b95eaca0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up tf-idf vectorizing\n",
    "tfidf_vectorizer = TfidfVectorizer(input='filename' , stop_words='english')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d417cc6-6093-4d4a-978d-9edcf3d9a2d0",
   "metadata": {},
   "source": [
    "N.B. This uses the default Scikit-Learn stopwords list. Try it first with the default, but you can use your own custom stopwords list if you want to modify your results (using the two cells bellow)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb53eca5-038d-49df-97bb-05828eb608fb",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Read in your txt file as list\n",
    "#with open('custom-stopwords.txt', 'r') as f:\n",
    "    #custom_stopwords = [s.rstrip('\\n') for s in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30975c40-c2e2-42eb-850b-d659021742c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up tf-idf vectorizing\n",
    "#tfidf_vectorizer = TfidfVectorizer(input='filename' , stop_words=custom_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c42c8b-2937-40c3-8aff-e24d7bbe22c5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Actually do the tf-idf vectorizing\n",
    "#(returns a tf-idf-weighted document-term matrix)\n",
    "tfidf_vector = tfidf_vectorizer.fit_transform(text_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360f80cf-95f8-4cc6-834b-de751332562b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a DataFrame out of the resulting tf–idf vectors, \n",
    "#setting the “feature names” (words in vocabulary) as columns \n",
    "#and the document titles as rows\n",
    "tfidf_df = pd.DataFrame(tfidf_vector.toarray(), \n",
    "                        index=text_titles, \n",
    "                        columns=tfidf_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b347064f-2f86-4229-b89a-3c8bfb638df8",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Re-organize DataFrame so that words are in rows rather than columns\n",
    "tfidf_df = tfidf_df.sort_index()\n",
    "stacked_tfidf_df = tfidf_df.stack().reset_index()\n",
    "stacked_tfidf_df = stacked_tfidf_df.rename(columns={0:'tfidf', 'level_0': 'document','level_1': 'term', 'level_2': 'term'})\n",
    "stacked_tfidf_df.sample(n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37661688-ac58-452d-91ef-781bba68bc75",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Create a dataframe that sorts the top 10 words with the highest tf–idf for every story\n",
    "num_top_words = 10\n",
    "top_tfidf = stacked_tfidf_df.sort_values(by=['document','tfidf'], ascending=[True,False]).groupby(['document']).head(num_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14728af-be4d-47ba-9e71-87ef223e5dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Zoom in on particular words\n",
    "#What documents have the given word in their top significant words?\n",
    "top_tfidf[top_tfidf['term'].str.contains('real')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07654272-4d37-4417-b0b8-065a131e4255",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Zoom in on particular document\n",
    "#What are the significant words for a given document?\n",
    "top_tfidf[top_tfidf['document'].str.contains('Drizzle')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c225ec8e-ab9b-4ee5-b74e-cf67c0c418d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#What are the top 20 significant words for the given document?\n",
    "(stacked_tfidf_df[stacked_tfidf_df['document']\n",
    "                  .str.contains('Drizzle')]\n",
    " .sort_values('tfidf', ascending=False)\n",
    " .head(20)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756b887c-46a4-4c92-8419-05f3b8c5e0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create bar plots of top 10 significant words for each document in corpus\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5,10))\n",
    "figure = sns.catplot(data=top_tfidf, row='document', x='tfidf', y='term', kind='bar', sharey=False)\n",
    "\n",
    "# Save figure\n",
    "#plt.savefig(\"sod-distinctive-words.pdf\", bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a060a6f2-f08e-4d80-b83a-636e4a58cd3e",
   "metadata": {},
   "source": [
    "_Acknowledgements_: This notebook is inspired by Melanie Walsh’s [_Introduction to Cultural Analytics & Python_](https://melaniewalsh.github.io/Intro-Cultural-Analytics/05-Text-Analysis/01-TF-IDF.html) and Matthew Lavin's [\"Analyzing Documents with TF-IDF](https://programminghistorian.org/en/lessons/analyzing-documents-with-tfidf)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
