{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0681f77-fb3f-4986-9df0-9282e3c4223f",
   "metadata": {},
   "source": [
    "# Keywords in Context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d5517c-a9cd-4c9f-be34-a868590e98da",
   "metadata": {},
   "source": [
    "### Finding sentences that contain a specific keyword\n",
    "\n",
    "We use SpaCy to break up our text into sentences and search for sentences that contain the specified keyword. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98c4960-a755-41c6-9e02-5e26ae43de6d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Import the libraries we need\n",
    "import spacy\n",
    "import re\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "#Download the language model you're interested in\n",
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21457b7b-a6b9-4205-aae3-c99af762df23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load language model\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "#Create spaCy document\n",
    "text = open('soderberg-corpus/1897_Drizzle.txt', encoding='utf-8').read()\n",
    "document = nlp(text)\n",
    "\n",
    "#Define a function to break text into sentences\n",
    "#and find sentences that contain a given keyword\n",
    "def find_sentences_with_keyword(keyword, document):\n",
    "        for sentence in document.sents:\n",
    "            sentence = sentence.text\n",
    "            if keyword.lower() in sentence.lower():\n",
    "                #Use the regex library to replace linebreaks and to make the keyword bolded, ignoring capitalization\n",
    "                sentence = re.sub('\\n', ' ', sentence)\n",
    "                sentence = re.sub(f\"{keyword}\", f\"**{keyword}**\", sentence, flags=re.IGNORECASE)\n",
    "                display(Markdown(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23652762-5db4-4f5f-b840-abe52b549feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call the function to find a specific keyword\n",
    "#You can change which keyword to look up\n",
    "find_sentences_with_keyword(keyword='sun', document=document)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca97a561-1004-4add-b81b-b9ba77aecc98",
   "metadata": {},
   "source": [
    "#### Finding sentences that contain a specific keyword in multiple files within a directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa823949-f730-44d7-b7ba-568b6a9a35c5",
   "metadata": {},
   "source": [
    "This does the same as above but for multiple files within a directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3898de3-8523-43f2-8562-0a38d9b246b7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Import the libraries we need\n",
    "import spacy\n",
    "import re\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "from pathlib import Path  \n",
    "import glob\n",
    "\n",
    "#Download the language model you're interested in\n",
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4896df35-4ee8-4a38-90c9-e4d993006efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load language model\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "#Define a function to break text into sentences\n",
    "#and find sentences with particular keyword\n",
    "def find_sentences_with_keyword(keyword, document):\n",
    "        for sentence in document.sents:\n",
    "            sentence = sentence.text\n",
    "            if keyword.lower() in sentence.lower():\n",
    "                #Use the regex library to replace linebreaks and to make the keyword bolded, ignoring capitalization\n",
    "                sentence = re.sub('\\n', ' ', sentence)\n",
    "                sentence = re.sub(f\"{keyword}\", f\"**{keyword}**\", sentence, flags=re.IGNORECASE)\n",
    "                display(Markdown(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5826721e-9d37-4d35-b776-a53288c1aed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set your filepath and define your text files\n",
    "filepath = 'soderberg-corpus/'\n",
    "text_files = glob.glob(f'{filepath}/*.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6221e718-573a-4308-a21c-6be2150b294c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Loop through the files and open as spaCy document\n",
    "#Then print for each document the sentences containing a given keyword\n",
    "#If document does not contain the keyword print the document name and 'none'\n",
    "#Change keyword to keyword you are looking for\n",
    "for file in text_files:\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "        print(file)\n",
    "        document = nlp(text)\n",
    "        kwic = find_sentences_with_keyword(keyword='sun', document=document)\n",
    "        print(kwic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff5605b-b187-4d47-9987-8b4bf14107d0",
   "metadata": {},
   "source": [
    "# Keywords in Context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9534889-ca9a-4b4e-aea2-13e5c01fe1e7",
   "metadata": {},
   "source": [
    "We can find a keyword’s immediate context, i.e.  its neighboring words to the left and right. To do so, we will first create a list of what’s called ngrams. “Ngrams” are any sequence of n tokens in a text. We’re going to use these n-grams to find the neighboring words that appear alongside particular keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56897f8f-b343-429f-a9d9-920bf97dc312",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7811c306-3a42-405e-9182-b286fd4ad3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set path to our file\n",
    "text_file = 'soderberg-corpus/1897_Drizzle.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37f2764-0a54-46e3-97cc-acfcff7e102c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a tokenizing function\n",
    "\"\"\"This function keeps only words, no numbers.\n",
    "It lowcases\n",
    "splits at and removes anything that is not a \"word\" character\n",
    "(i.e. a letter or digit or underbar)\n",
    "so it will split at and remove whitspace and punctuation\n",
    "Then keeps only alphabetic characters \n",
    "(i.e. remove numbers) with .isalpha()\n",
    "\"\"\"\n",
    "\n",
    "def tokenize(text):\n",
    "    lowercase_text = text.lower()\n",
    "    split_words = re.split(r'\\W+', lowercase_text)\n",
    "    tokenized = [word for word in split_words if word.isalpha()]\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65a55ad-2869-4464-bca4-9f609900d2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Open the file and tokenize it \n",
    "#(creates a list of all the words/tokens in all_words)\n",
    "with open(text_file, 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "    all_words = tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe69e3f-5f29-4fca-87a9-6b4a02bac6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a function to remove stopwords from your list of tokens\n",
    "#using custom stopwords list\n",
    "with open(\"custom-stopwords.txt\", \"r\") as file_object:\n",
    "    custom_stopwords = file_object.read()\n",
    "\n",
    "def remove_stopwords(list_of_tokens, stopwords):\n",
    "    return [token for token in list_of_tokens if token not in stopwords]\n",
    "\n",
    "all_words_no_stop = remove_stopwords(all_words, custom_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab4bce6-0ff7-4f27-b276-865ba3644a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a function to return list of ngrams\n",
    "def make_ngrams(tokens, n):\n",
    "    ngrams = []\n",
    "    for i in range(len(tokens)-(n-1)):\n",
    "        ngrams.append(tokens[i:i+n])\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6866076-ae89-4695-876d-a47312d1a0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a function to create a dictionary from n-grams, using middle word as the key.\n",
    "def ngrams_to_dictionary(ngrams):\n",
    "    keyindex = len(ngrams[0]) // 2\n",
    "\n",
    "    ngram_dictionary = {}\n",
    "\n",
    "    for ngram in ngrams:\n",
    "        if ngram[keyindex] not in ngram_dictionary:\n",
    "            ngram_dictionary[ngram[keyindex]] = [ngram]\n",
    "        else:\n",
    "            ngram_dictionary[ngram[keyindex]].append(ngram)\n",
    "    return ngram_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66862fa8-4c66-4e28-a80f-f365f36ee95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call your functions\n",
    "#Change the number to change your context window\n",
    "#(i.e. how many words you want around the keyword)\n",
    "\n",
    "ngrams = make_ngrams(all_words_no_stop, 6)\n",
    "\n",
    "keyword_in_context = ngrams_to_dictionary(ngrams)\n",
    "\n",
    "keyword_in_context['sun']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c640578-1bb7-4ce9-a3ab-86b017e337ce",
   "metadata": {},
   "source": [
    "#### Most Frequent Neighboring Words\n",
    "\n",
    "What if we want to find the most frequent neighoring words that appear close to a particular keyword?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc3b78f-cb55-4895-b0b0-a601fb78c88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a function to return most frequent words \n",
    "#that appear next to a particular keyword\n",
    "def get_neighbor_words(keyword, ngrams):\n",
    "    \n",
    "    neighbor_words = []\n",
    "    keyword = keyword.lower()\n",
    "    \n",
    "    for ngram in ngrams:\n",
    "        if keyword in ngram:\n",
    "            for word in ngram:\n",
    "                if word != keyword:\n",
    "                        neighbor_words.append(word)\n",
    "    return Counter(neighbor_words).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbaacd19-8d8d-4a4d-ad4a-8a1b0e399ef9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Call your functions\n",
    "ngrams = make_ngrams(all_words_no_stop, 6)\n",
    "\n",
    "keyword_in_context = ngrams_to_dictionary(ngrams)\n",
    "\n",
    "get_neighbor_words('sun', ngrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ede2710-9c81-44bd-bdd1-d23033c398d0",
   "metadata": {},
   "source": [
    "### Keywords in Context across multiple files within a directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd351ec-27fd-41cf-b221-39862d5b4926",
   "metadata": {},
   "source": [
    "Same as above but for multiple files within a directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a137573-e357-48a5-8236-57921adb138f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Set path to your corpus\n",
    "#define that you want to analyze all .txt files in the directory\n",
    "directory_path = 'soderberg-corpus'\n",
    "text_files = glob.glob(f'{directory_path}/*.txt')\n",
    "print(text_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e0e54e-214b-40c7-854b-150082ea3b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize the text files and append tokens to all_docs\n",
    "#This create a list of lists of all tokens from all the documents\n",
    "all_docs = []\n",
    "\n",
    "def tokenize(text):\n",
    "    lowercase_text = text.lower()\n",
    "    split_words = re.split(r'\\W+', lowercase_text)\n",
    "    tokenized = [word for word in split_words if word.isalpha()]\n",
    "    return tokenized\n",
    "\n",
    "for filepath in text_files:\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "        tokenized_text = tokenize(text)\n",
    "        all_docs.append(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c402171-1519-404e-8284-8edef88768e9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Remove stopwords with custom stopwords\n",
    "\n",
    "#Read in custom stopwords txt as list\n",
    "with open(\"custom-stopwords.txt\", \"r\") as file_object:\n",
    "    custom_stopwords = [s.rstrip('\\n') for s in file_object.readlines()] \n",
    "\n",
    "#Define function to remove stopwrods\n",
    "def remove_stopwords(list_of_tokens, stopwords):\n",
    "    return [token for token in list_of_tokens if token not in stopwords]\n",
    "\n",
    "#Loop over all_docs to remove stopwords\n",
    "all_docs_no_stop = []\n",
    "\n",
    "for file in all_docs: \n",
    "    nostop = remove_stopwords(file, custom_stopwords)\n",
    "    all_docs_no_stop.append(nostop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a02e634-1b4c-4de3-8924-68a9fc64e140",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a function to return list of ngrams\n",
    "def make_ngrams(tokens, n):\n",
    "    ngrams = []\n",
    "    for i in range(len(tokens)-(n-1)):\n",
    "        ngrams.append(tokens[i:i+n])\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e989e4-127f-4016-89a9-b0c2295c467f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a function to create a dictionary from n-grams, using middle word as the key.\n",
    "#To figure out the keyword for each n-gram we can use the index positions of the list.\n",
    "def ngrams_to_dictionary(ngrams):\n",
    "    keyindex = len(ngrams[0]) // 2\n",
    "\n",
    "    ngram_dictionary = {}\n",
    "\n",
    "    for ngram in ngrams:\n",
    "        if ngram[keyindex] not in ngram_dictionary:\n",
    "            ngram_dictionary[ngram[keyindex]] = [ngram]\n",
    "        else:\n",
    "            ngram_dictionary[ngram[keyindex]].append(ngram)\n",
    "    return ngram_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e10c8e0-f9fd-4d5c-840e-b1ea4ce96626",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loop through the files and append the dictionaries for each file\n",
    "#to a list called keywords\n",
    "#Change the number (6) to change the size of the context window \n",
    "#(i.e. the number of words around the keyword)\n",
    "\n",
    "keywords = []\n",
    "\n",
    "for file in all_docs_no_stop:\n",
    "        ngrams = make_ngrams(file, 6)\n",
    "        keywords_in_context = ngrams_to_dictionary(ngrams)\n",
    "        keywords.append(keywords_in_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cece34a-b5c8-4e40-a93c-b40e12f77f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a function that will loop through the list of ngram dictionaries\n",
    "#and print out a given keyword with its ngrams\n",
    "#or print a line that the keyword is not in the dictionary\n",
    "def lookup_keyword(kw, dictionaries):\n",
    "    for i in range(len(dictionaries)):\n",
    "        text_name = text_files[i]\n",
    "        dictionary = dictionaries[i]\n",
    "        if kw in dictionary:\n",
    "            print(text_name, dictionary[kw], \"\\n\")\n",
    "        else:\n",
    "            print(f\"{kw} not in file: {text_name}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871ca9f3-5af8-4acd-900a-68b3f900c197",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Look up the words that appear next to a given keyword \n",
    "#for each text in the corpus \n",
    "lookup_keyword('god', keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8feb1eb-3f4e-4171-a43c-c940f4316e84",
   "metadata": {},
   "source": [
    "#### Most Frequent Neighboring Words across multiple files within a directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771d5bc5-9e87-45e0-a45b-627dd76df5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd60587-e619-484e-9e38-a125e2e4273d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look up most frequent words that appear next to a given keyword\n",
    "\n",
    "#Define a function to return most frequent words \n",
    "#that appear next to a particular keyword\n",
    "def get_neighbor_words(keyword, ngrams):\n",
    "    \n",
    "    neighbor_words = []\n",
    "    keyword = keyword.lower()\n",
    "    \n",
    "    for ngram in ngrams:\n",
    "        if keyword in ngram:\n",
    "            for word in ngram:\n",
    "                if word != keyword:\n",
    "                        neighbor_words.append(word)\n",
    "    return Counter(neighbor_words).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5154674a-89ac-4848-87e2-bff1441027d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loop through the files and append the ngrams for each file\n",
    "#to a list called all_ngrams\n",
    "#Change the number to change the context window\n",
    "\n",
    "all_ngrams = []\n",
    "\n",
    "for file in all_docs_no_stop:\n",
    "    ngrams = make_ngrams(file, 6)\n",
    "    all_ngrams.append(ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cae38f-0446-4e6d-accc-ff27a548a360",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a function to loop through ngrams above\n",
    "# and look up most common neighbor words for a given keyword\n",
    "def lookup_neighbor_words(keyword, ngram_list):\n",
    "    for i in range(len(ngram_list)):\n",
    "        text_name = text_files[i]\n",
    "        text_ngrams = ngram_list[i]\n",
    "        print(text_name, get_neighbor_words(keyword, text_ngrams), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66ccc62-4d75-4603-b3f6-31f04632e7b6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Look up the most common neighbor words for a given keyword\n",
    "#for each text in the corpus\n",
    "lookup_neighbor_words('sun', all_ngrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cdcced-fe99-43fb-9ba7-54a755b01998",
   "metadata": {},
   "source": [
    "_Acknowledgements_: This notebook is inspired by Melanie Walsh’s [_Introduction to Cultural Analytics & Python_](https://melaniewalsh.github.io/Intro-Cultural-Analytics/05-Text-Analysis/Multilingual/Chinese/03-POS-Keywords-Chinese.html#keyword-extraction) and William Turkel and Adam Crymble's [\"Keywords in Context (using n-grams) with Python\"](https://programminghistorian.org/en/lessons/keywords-in-context-using-n-grams)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
