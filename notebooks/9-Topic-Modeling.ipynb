{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e10ca115-5b74-4d50-83d3-bde2d4b196cf",
   "metadata": {},
   "source": [
    "# Topic Modeling in Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5171e988-4fd4-424c-8696-077f941442f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import LdaModel\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "from pathlib import Path  \n",
    "import glob\n",
    "\n",
    "import re\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f6298b-aee6-4ed8-9c5d-7d91a3834f5e",
   "metadata": {},
   "source": [
    "## Preprocessing steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2249c7e0-882b-4667-9310-6466fdb2fb1d",
   "metadata": {},
   "source": [
    "In these initial steps we're preparing our corpus in order to create topic models using Gensim. \n",
    "\n",
    "We follow some preprocessing steps already familiar to us by now (tokenzing, removing stopwords, creating bigrams and trigrams) and we're also structuring the data in a certain way that Gensim requires in order to train our LDA models (as a dictionary assigning unique IDs to words and as vector representations ie. each document is represented by a vector where each vector element is the frequency count of a particular word in that document and the frequency count for each word can be mapped onto the unique ID in the dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e2982b-9a31-48e7-a3b8-3332c1471de2",
   "metadata": {},
   "source": [
    "**Tokenize your text either using gensim built-in tokenizing or using your own tokenizing function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d49fb1-7163-4c29-bb71-65214999c69e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tokenize using gensim built-in tokenization\n",
    "\n",
    "#Loop through the texts and tokenize them with gensim tokenizing function\n",
    "directory_path = 'soderberg-corpus/'\n",
    "all_docs = []\n",
    "\n",
    "for filepath in Path(directory_path).glob(\"*.txt\"):\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "        tokenized_text = gensim.utils.simple_preprocess(text)\n",
    "        all_docs.append(tokenized_text)\n",
    "\n",
    "#See the first document as tokenized list of words\n",
    "all_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcfca5c-e9ad-47d6-94e3-c6f761da1462",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tokenize using cutsom tokenizing function\n",
    "\n",
    "#Put all texts into a single list\n",
    "#Loop through the texts and tokenize them with custom tokenizing function\n",
    "from pathlib import Path\n",
    "directory_path = 'soderberg-corpus/'\n",
    "all_docs = []\n",
    "\n",
    "def tokenize(text):\n",
    "    lowercase_text = text.lower()\n",
    "    split_words = re.split(r'\\W+', lowercase_text)\n",
    "    tokenized = [word for word in split_words if word.isalpha()]\n",
    "    return tokenized\n",
    "\n",
    "for filepath in Path(directory_path).glob(\"*.txt\"):\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "        tokenized_text = tokenize(text)\n",
    "        all_docs.append(tokenized_text)\n",
    "\n",
    "#See the first document as tokenized list of words\n",
    "all_docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1b8412-95e7-42f8-ba06-7b879d109ac1",
   "metadata": {},
   "source": [
    "**Remove stopwords**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af04f9b7-d48e-4c02-a93c-9c5c51889870",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Load custom stopwords list\n",
    "#open your txt file and convert to a Python list\n",
    "with open(\"custom-stopwords.txt\", \"r\") as file_object:\n",
    "    custom_stopwords = [s.rstrip('\\n') for s in file_object.readlines()] \n",
    "\n",
    "custom_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc751e46-b0a7-49c4-8999-369164dc942f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(list_of_tokens, stopwords):\n",
    "    return [token for token in list_of_tokens if token not in stopwords]\n",
    "\n",
    "all_docs_no_stop = []\n",
    "\n",
    "for file in all_docs: \n",
    "    nostop = remove_stopwords(file, custom_stopwords)\n",
    "    all_docs_no_stop.append(nostop)\n",
    "    \n",
    "all_docs_no_stop[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1411f748-70ad-4bf8-9b24-702c9b671067",
   "metadata": {},
   "source": [
    "**Creating Bigrams and Trigrams**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27885150-bcd7-4729-95ec-9c7850d907d8",
   "metadata": {},
   "source": [
    "Bigrams are two words frequently occurring together that need to be grouped together to make sense (e.g. \"black hole\", \"European Union\"). Trigrams are 3 words frequently occurring together that need to be grouped together to make sense. Identifying bigrams and trigrams in our corpus will improve the quality of the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924673bd-be4b-402c-a9f0-14de37a00019",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Identify bigrams and trigrams\n",
    "# min_count: minimum number of times words occur together to be considered a bigram\n",
    "# threshhold: the higher the number the fewer number of ngrams will be identified\n",
    "bigram = gensim.models.Phrases(all_docs_no_stop, min_count=5, threshold=100)\n",
    "trigram = gensim.models.Phrases(bigram[all_docs_no_stop], threshold=100)\n",
    "\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "data_bigrams = make_bigrams(all_docs_no_stop)\n",
    "data_bigrams_trigrams = make_trigrams(data_bigrams)\n",
    "\n",
    "#You can find the ngram by searching for words linked with underscore \n",
    "#(command + F and search for underscore)\n",
    "#If you're not staisfied with the bigrams you're getting (capturing too many\n",
    "#or too few then modify the min_count and threshhold parameters\n",
    "print(data_bigrams_trigrams[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b67fee-cab8-4657-bea8-c3e792d873ea",
   "metadata": {},
   "source": [
    "**Creating a dictionary representation of the documents**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b806b9c3-430f-4d14-9d0c-95ed0180a23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "# We assign a unique integer ID (key) to all words in vocabulary of the corpus\n",
    "id2word = corpora.Dictionary(data_bigrams_trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40979d6-df06-4a9c-a89e-728c50465ae8",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We can use token2id to see the mapping between words and their ids\n",
    "print(id2word.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2836df39-088c-4f63-bdca-d05252a5863b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a Corpus\n",
    "#Vectorize the texts\n",
    "#(ie. count the number of occurrences of each word in each text)\n",
    "#and associcate these frequency counts\n",
    "#with the word ID in the dictionary\n",
    "\n",
    "corpus = []\n",
    "for text in data_bigrams_trigrams:\n",
    "    new = id2word.doc2bow(text)\n",
    "    corpus.append(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211c6664-f4c0-477e-94c1-86fa4ca323e5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Print corpus (vector representation) for first document\n",
    "#You will see a list of unique word ID, and its frequency\n",
    "print (corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d32d6fa-657c-410f-a68d-b79153b4a4d7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Human readable format of corpus (term and its frequency)\n",
    "#Lists the words and their frequency for first document\n",
    "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936e67a3-6d07-46b9-95cb-ddfeb4b0a0d1",
   "metadata": {},
   "source": [
    "## Training an LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a271eb8-c04b-41b8-a502-fa4b40ea6d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Parameters: \n",
    "We train our model on the corpus and dictionary we created above\n",
    "\n",
    "num_topics: the number of topics you want\n",
    "can increase and descrease until find topics useful for analysis\n",
    "\n",
    "passes: total number of training passes\n",
    "(ie. how often we train the model on the entire corpus)\n",
    "iterations: controls how often we repeat a particular loop over each document.\n",
    "It is important to set the number of “passes” and \"iterations\" high enough \n",
    "so that by the final passes, most of the documents have converged.\n",
    "\n",
    "chunksize: the number of documents to be used in each training chunk.\n",
    "I’ve set chunksize = 100, which is more than the amount of documents, \n",
    "so I process all the data in one go.\n",
    "\n",
    "alpha = 'auto' and eta = 'auto': essentially we are automatically learning \n",
    "two parameters in the model that we usually would have to specify explicitly.\n",
    "\n",
    "Experiment with changing the parameters (especially num_topics) until you get meaningful results\n",
    "Link to documentation: https://radimrehurek.com/gensim/models/ldamodel.html\n",
    "\"\"\"\n",
    "\n",
    "# Train LDA model\n",
    "lda_model = LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=id2word,\n",
    "    num_topics=8,\n",
    "    chunksize=100,\n",
    "    passes=200,\n",
    "    iterations=500,\n",
    "    alpha='auto',\n",
    "    eta='auto',\n",
    "    per_word_topics=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064cfac8-2f60-442b-b929-1cb0cbb87926",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Topics (probability distributions of words across the corpus)\n",
    "#This list the topics (Topic 0,1,2 etc.)\n",
    "#and print the list of words most characteristic for each topic\n",
    "#preceded by its proability score (how strongly it is characteristic of the topic)\n",
    "#change the num_words to get more or less words for each topic\n",
    "pprint(lda_model.print_topics(num_words = 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a477a2a9-a0ad-4344-9eb4-deaf7f38862a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Distributions of topics over documents: \n",
    "#what topics are associated with each document\n",
    "#This returns a list of each document which lists the most characteristic topics \n",
    "#for that document and their weight of association (topic proability) with that document\n",
    "\n",
    "topics_per_document=[lda_model.get_document_topics(item, minimum_probability=None) for item in corpus]\n",
    "topics_per_document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf5b6c2-56fd-4d4d-b78b-0fc77d918ce2",
   "metadata": {},
   "source": [
    "## Vizualizing the models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9f03d8-3151-45b8-a5bb-8a93602ec47e",
   "metadata": {},
   "source": [
    "**Visualizing using pyLDAvis**\n",
    "\n",
    "Each bubble on the left-hand side of the plot represents a topic. The larger the bubble, the more prevalent is that topic and the more documents associated with that topic.\n",
    "\n",
    "A good topic model will have fairly big, non-overlapping bubbles scattered throughout the chart instead of being clustered in one quadrant. Although slightly overlapping topics is not a bad thing: they reveal connections between topics.\n",
    "\n",
    "A model with too many topics, will typically have many overlaps, small sized bubbles clustered in one region of the chart.\n",
    "\n",
    "If you move the cursor over the bubbles, the words and bars on the right-hand side will update. These words are the words characteristic of that topic.\n",
    "\n",
    "This visualization can give you sense of how you can tune your models by adjusting your parameters (e.g. increasing or decreasing number of topics). If words are not meaningful can also add them to custom stopwords list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10afd3dd-45ef-42ab-bf39-4dd082af715b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a174e470-157f-4d53-b660-03c3937d9a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "lda_display = gensimvis.prepare(lda_model, corpus, id2word, mds='mmds')\n",
    "pyLDAvis.display(lda_display)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da2fce3-a58f-4dc1-8b60-2043af40950b",
   "metadata": {},
   "source": [
    "**Visualize which topics are associated with each document in a heatmap**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c3ba02-17b9-41aa-81cc-832473c81a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a matrix from topics_per_document results above\n",
    "#The number of topics you used to train your model above\n",
    "num_topics = 8\n",
    "\n",
    "num_texts=len(topics_per_document)\n",
    "\n",
    "document_topic_matrix = np.zeros((num_texts, num_topics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7679e210-3e2f-4e15-b539-f0216c2abba9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i, document in enumerate(topics_per_document):\n",
    "    for (topic_index, probability) in document: \n",
    "        document_topic_matrix[i][topic_index]=probability\n",
    "\n",
    "document_topic_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e00cb7a-211e-4abb-8ec0-aa153e285371",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Create columns that match the topics\n",
    "topic_names = [\"Topic {}\".format(i) for i in range(num_topics)]\n",
    "\n",
    "#Get file names\n",
    "filepath = 'soderberg-corpus/'\n",
    "text_files = [s.lstrip(filepath) for s in glob.glob(filepath + '*.txt')]\n",
    "\n",
    "#Make dataframe \n",
    "df = pd.DataFrame(document_topic_matrix, columns=topic_names, index=text_files)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262ab281-e43f-4aaf-b97c-c0241f5b01d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Heatmap of topic distributions for each document\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "figure = sns.heatmap(df, annot=True, cmap=\"magma_r\", ax = ax)\n",
    "\n",
    "# Uncommment lines below to save the figure\n",
    "#plt.savefig(\"Topics-heatmap.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105c50d0-1b97-4d98-b7a8-4f85f5cd7793",
   "metadata": {},
   "source": [
    "**Visualize how topics change across time**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd24de4-d984-453a-bbb2-172aacc7739d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Sort the index by date in the document name\n",
    "df_sorted = df.sort_index()\n",
    "df_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9aaa91a-4153-41f5-a1e5-86025ede3976",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Heatmap of topic distributions for each document sorted by date\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "figure = sns.heatmap(df_sorted, annot=True, cmap=\"magma_r\", ax = ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93101091-1493-4b48-9496-0e178578c03d",
   "metadata": {},
   "source": [
    "_Acknowledgements_: This notebook is inspired by William Mattingly's [\"Topic Modeling and Text Classification with Python\" tutorial](https://www.youtube.com/watch?v=N0crN8YnF8Y&list=PL2VXyKi-KpYttggRATQVmgFcQst3z6OlX)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
